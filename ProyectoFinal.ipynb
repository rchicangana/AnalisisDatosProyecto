{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rchicangana/AnalisisDatosProyecto/blob/IVAN/ProyectoFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ESTADO DEL ARTE\n",
        "\n",
        "- La imposibilidad de almacenar la energía eléctrica hace de la predicción una herramienta indispensable a la hora de gestionar eficientemente su producción. La modelización de la\n",
        "demanda de energía eléctrica permite al ente productor contar con un conocimiento más refinado de los mercados y de los usuarios del sistema, así como con una mejor posición mediante la reducción de la incertidumbre en la toma de decisiones.  \n",
        "\n",
        "- Se consultaron las Bases de Datos SciELO (Scientific Electronic Library Online) y RedAlyc (Red de Revistas Científicas de América Latina y el Caribe, España y Portugal), EBSCO, Springer Link, IEEE Xplorer y Google Académico.\n",
        "\n",
        "- Para medir la relevancia y pertinencia de los documentos se tomaron como indicadores: que los artículos presentaran las técnicas utilizadas basadas en Inteligencia Artificial, así como metodologías y estudios de casos concretos dentro de las áreas que abarca la ingeniería eléctrica.\n",
        "\n",
        "- Destacan dentro de las áreas de impacto la predicción de la producción y consumo eléctrico, lo cual se refiere a estudios realizados para establecer valores futuros del consumo eléctrico de una edificación particular, un complejo habitacional, industrial o incluso una ciudad o país completos. Como se indicó previamente, la adecuada predicción de estos datos es de gran importancia para planificar las fuentes de energía eléctrica y los costos asociados para los consumidores.\n",
        "\n",
        "- La inteligencia artificial ofrece en la actualidad una diversidad de técnicas para la predicción de variables. Es la presencia de estas técnicas, en la forma de algoritmos implementados, que permiten relacionar las publicaciones con la inteligencia artificial. La diversidad de opciones de algoritmos generados se refleja en los artículos analizados\n",
        "Las redes neuronales artificiales y las máquinas de vectores de soporte son las predominantes entre todas las utilizadas. Es importante destacar que es una práctica común la utilización de más de una técnica en un estudio de predicción, para realizar comparaciones y proponer la adopción de aquella técnica que haya dado mejores resultados. tambien se comentan tecnicas como Gradient Boosting y Modelos de Markov.\n",
        "\n",
        "Entre las variables utilizadas para los analisis se encuentran principalmente la Temperatura, Humedad y la fecha de medicion\n",
        "\n",
        "\n",
        "-Los resultados muestran una tendencia creciente en el interés de aprovechar las posibilidades de la inteligencia artificial a la tarea de predicción de variables, y confirman la mayor representatividad de los casos en la predicción de producción y consumo energético, y en variables relacionadas con energías renovables, tales como las condiciones del viento y de energía solar. La predominancia del idioma de las publicaciones es el inglés, y en países como China, Estados Unidos y Colombia es donde se ha dado la mayor cantidad de estudios.\n",
        "\n",
        "-Los artículos más citados en la temática tratan sobre predicción de consumo eléctrico en edificaciones particulares, y de predicción de energía solar. Las predicciones son de utilidad para labores de planificación, estimación de costos y previsión de escenarios de inversión y aprovechamiento de recursos.\n",
        "\n",
        "-El análisis realizado en el estado del arte deja patente la actualidad y necesidad del tema de inteligencia artificial para la ingeniería eléctrica, especialmente el conocimiento de las técnicas más utilizadas para los sistemas de predicción determinadas en el presente estudio, como lo son las redes neuronales artificiales, las máquinas de vectores de soporte y los árboles de decisión. Por otra parte, el creciente interés que muestran las referencias puede interpretarse como un área de oportunidad para aprovechar las ventajas de los estudios de predicción de variables con los nuevos métodos de inteligencia artificial en lugares donde aún no se han implementado.\n",
        "\n",
        "\n",
        "## Articulos consultados\n",
        "\n",
        "- https://www.sciencedirect.com/science/article/pii/S2352484724007315\n",
        "- https://ciencialatina.org/index.php/cienciala/article/view/2502\n",
        "- https://www.revistaespacios.com/a17v38n22/a17v38n21p03.pdf\n",
        "- http://www.scielo.org.co/scielo.php?script=sci_arttext&pid=S1794-12372016000200009\n",
        "- https://pmc.ncbi.nlm.nih.gov/articles/PMC10948898/\n",
        "- https://www.redalyc.org/pdf/462/46247652001.pdf\n",
        "- https://www.kaggle.com/datasets/fedesoriano/electric-power-consumption\n",
        "\n",
        "### Los articulos mas citados en la publicaciones fueron\n",
        "\n",
        "\n",
        "| Autores                                                       | Título del   artículo                                                                                                                                                                                        | Nombre de   la revista/publicación                           | Año  | Cantidad de citas |\n",
        "|---------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------|------|-------------------|\n",
        "| Gonzalez,   P. A., y Zamarreno, J. M..                        | Prediction of hourly energy   consumption in buildings based on a feedback artificial neural network                                                                                                         | Energy and buildings                                         | 2005 | 392               |\n",
        "| Jain,   R. K., Smith, K. M., Culligan, P. J., y Taylor, J. E. | Forecasting energy consumption   of multi-family residential buildings using support vector regression:   Investigating the impact of temporal and spatial monitoring granularity on   performance accuracy. | Applied Energy                                               | 2014 | 379               |\n",
        "| Sharma,   N., Sharma, P., Irwin, D., y Shenoy, P.             | Predicting solar generation from   weather forecasts using machine learning.                                                                                                                                 | IEEE international conference on   smart grid communications | 2011 | 370               |\n",
        "| Yang,   J., Rivard, H., y Zmeureanu, R.                       | On-line building energy   prediction using adaptive artificial neural networks.                                                                                                                              | Energy and buildings,                                        | 2005 | 368               |\n",
        "| Ekici,   B. B., y Aksoy, U. T.                                | Prediction of building energy   consumption by using artificial neural networks.                                                                                                                             | Advances in Engineering   Software.                          | 2009 | 358               |\n",
        "| Mocanu,   E., Nguyen, P. H., Gibescu, M., y Kling, W. L.      | Deep learning for estimating   building energy consumption.                                                                                                                                                  | Sustainable Energy, Grids and   Networks.                    | 2016 | 342               |\n",
        "| Edwards,   R. E., New, J., y Parker, L. E.                    | Predicting future hourly   residential electrical consumption: A machine learning case study.                                                                                                                | Energy and Buildings.                                        | 2012 | 275               |\n",
        "| Ahmad,   M. W., Mourshed, M., y Rezgui, Y.                    | Trees vs Neurons: Comparison   between random forest and ANN for high-resolution prediction of building   energy consumption.                                                                                | Energy and Buildings                                         | 2017 | 263               |\n",
        "| Tüfekci,   P.                                                 | Prediction of full load   electrical power output of a base load operated combined cycle power plant   using machine learning methods.                                                                       | International Journal of   Electrical Power & Energy Systems | 2014 | 241               |\n",
        "| Candanedo,   L. M., Feldheim, V., y Deramaix, D.              | Bélgica                                                                                                                                                                                                      | Energy and buildings                                         | 2017 | 2                 |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVIuGSCNquud"
      },
      "source": [
        "# 1  Introduccion\n",
        "\n",
        "El análisis exploratorio de datos es una etapa fundamental en cualquier proyecto de Inteligencia Artificial y Ciencia de Datos, puesto que permite entender las características y patrones en los datos, así como identificar relaciones entre variables.\n",
        "\n",
        "A continuación se presenta un estudio basado en datos de consumo de energía eléctrica horaria registrados en Cali, Colombia, junto con las temperaturas correspondientes a cada hora. El objetivo principal es explorar la relación entre estas dos variables y, posteriormente, aplicar diferentes modelos para realizar el pronósticos de corto plazo del demanda de energía."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyGVSbOMq_0K"
      },
      "source": [
        "# 2  Contexto\n",
        "\n",
        "La demanda de energía eléctrica en una ciudad se ve inlfuenciada por muchos factores, como la temperatura, factores sociales, cronograma de produccion de grandes clientes, entre otros.\n",
        "\n",
        "En regiones con climas variables, como es el caso de muchas ciudades Colombianas, la demanda es afectada por el uso intensivo de sistemas de refrigeración en épocas calurosas.\n",
        "\n",
        "Los datos utilizados en este análisis incluyen mediciones horarias del consumo de energía y la temperatura registrada en la misma franja horaria, tipos de dia, mes y año. Lo que permite explorar la correlación entre estas variables.\n",
        "\n",
        "Adicionalmente, se busca evaluar el desempeño de diferentes modelos como regresiones lineales, redes neuronales y modelos basados en árboles de decisión, para predecir el consumo de energía en un horizonte de corto plazo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvcVK9n2rB4e"
      },
      "source": [
        "## 2.1  Descripción de los campos\n",
        "\n",
        "- FECHA: Fecha en formato ISO de la lectura del consumo\n",
        "- TEMPERATURA: Temperatura en grados Centigrados al momento de la toma del dato\n",
        "- CONSUMO: Variable objetvio medido en MVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2eSjd_Fz7WiZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import holidays\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "df = pd.read_csv('Data/datasetEmcali.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"primeras filas \\n\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "print(\"se describen las columnas \\n\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"dimensiones \\n\")\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUTgDLYErbbx"
      },
      "source": [
        "# 3  Análisis exploratorio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1t0tE05rdWs"
      },
      "source": [
        "## 3.1  Análisis de estructura\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSaKvv-pKkYL"
      },
      "source": [
        "Se realiza un analisis de la forma del dataset, se relacionan tipos de datos, estadisticas descriptivas y valores nulos.\n",
        "\n",
        "Se adicionan columnas, basadas en la fecha, con la hipotesis que esto permitira al modelo entender e interpretar mejor los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tratamiento de dataset Humedad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Defino el formato de la fecha\n",
        "df[\"Date\"] = pd.to_datetime(df[\"FECHA\"], format=\"mixed\", errors=\"coerce\")\n",
        "\n",
        "# Cargar datasets de humedad\n",
        "humedad_files = glob.glob(\"Data/reporte_sisaire-humedad-*.csv\")\n",
        "df_humedad_list = [pd.read_csv(file) for file in humedad_files]\n",
        "df_humedad = pd.concat(df_humedad_list)\n",
        "\n",
        "# Convertir fechas en el dataset de humedad\n",
        "df_humedad[\"Fecha inicial\"] = pd.to_datetime(df_humedad[\"Fecha inicial\"], format=\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "# Promediar la humedad por hora\n",
        "df_humedad_avg = df_humedad.groupby(\"Fecha inicial\")[\"HAire2\"].mean().reset_index()\n",
        "df_humedad_avg.rename(columns={\"Fecha inicial\": \"Date\", \"HAire2\": \"Humidity\"}, inplace=True)\n",
        "\n",
        "df_humedad_avg.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tratamiento de dataset Temperatura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datasets de velocidad del viento\n",
        "viento_files = glob.glob(\"Data/reporte_sisaire-vViento-*.csv\")\n",
        "df_viento_list = [pd.read_csv(file) for file in viento_files]\n",
        "df_viento = pd.concat(df_viento_list)\n",
        "\n",
        "# Convertir fechas en el dataset de viento\n",
        "df_viento[\"Fecha inicial\"] = pd.to_datetime(df_viento[\"Fecha inicial\"], format=\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "# Promediar la velocidad del viento por hora\n",
        "df_viento_avg = df_viento.groupby(\"Fecha inicial\")[\"VViento\"].mean().reset_index()\n",
        "df_viento_avg.rename(columns={\"Fecha inicial\": \"Date\", \"VViento\": \"Wind\"}, inplace=True)\n",
        "\n",
        "df_viento_avg.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unión de datos de dos DataFrames, df_humedad_avg y df_viento_avg, con el DataFrame principal df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_humedad_avg[\"Date\"] = pd.to_datetime(df_humedad_avg[\"Date\"])\n",
        "df_viento_avg[\"Date\"] = pd.to_datetime(df_viento_avg[\"Date\"])\n",
        "\n",
        "df = df.merge(df_humedad_avg, on=\"Date\", how=\"left\", suffixes=(\"\",\"_extra\"))\n",
        "df = df.merge(df_viento_avg, on=\"Date\", how=\"left\" , suffixes=(\"\",\"_extra\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se realiza la Conversion de Columnas a Valores Numericos y se eliminnan las filas que tienen valores nulos en 'FECHA' y 'CONSUMO', 'TEMPERATURA', 'HUMEDAD', 'VIENTO'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Consumption'] = pd.to_numeric(df['CONSUMO'], errors='coerce')\n",
        "\n",
        "df['Temperature'] = pd.to_numeric(df['TEMPERATURA'], errors='coerce')\n",
        "\n",
        "df['Humidity'] = pd.to_numeric(df['Humidity'], errors='coerce')\n",
        "\n",
        "df['Wind'] = pd.to_numeric(df['Wind'], errors='coerce')\n",
        "\n",
        "# Elimino las filas que tienen valores nulos\n",
        "\n",
        "df = df.dropna(subset=['Date', 'Consumption', 'Temperature', 'Humidity', 'Wind'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se Verifica si la fecha es un dia festivo en Colombia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def es_festivo_colombia(fecha):\n",
        "    \"\"\"Verifica si una fecha es festivo en Colombia.\"\"\"\n",
        "    co_holidays = holidays.CO(years=fecha.year)\n",
        "    return fecha in co_holidays\n",
        "df['Is_holiday'] = df['Date'].apply(es_festivo_colombia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se decompone la fecha, se crean nuevas columnas basados en sus componentes, Esto puede ser útil para análisis de series temporales o para extraer información adicional de las fechas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# se descompone la fecha\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['Hour'] = df['Date'].dt.hour\n",
        "df['Dayofweek'] = df['Date'].dt.dayofweek\n",
        "df['Dayofyear'] = df['Date'].dt.dayofyear\n",
        "df['Weekofyear'] = df['Date'].dt.isocalendar().week\n",
        "df['Is_month_end'] = df['Date'].dt.is_month_end\n",
        "df['Is_month_start'] = df['Date'].dt.is_month_start\n",
        "df['Is_quarter_end'] = df['Date'].dt.is_quarter_end\n",
        "df['Is_quarter_start'] = df['Date'].dt.is_quarter_start\n",
        "df['Is_year_end'] = df['Date'].dt.is_year_end\n",
        "df['Is_year_start'] = df['Date'].dt.is_year_start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se eliminan columnas innecesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eliminar la columna 'FECHA', ya que no es necesaria\n",
        "df = df.drop('Date', axis=1)\n",
        "df = df.drop('FECHA', axis=1)\n",
        "df = df.drop('TEMPERATURA', axis=1)\n",
        "df = df.drop('CONSUMO', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HXUvfE-D9Ngq",
        "outputId": "70912245-9da2-4cfb-ab0e-1465c2da52a8"
      },
      "outputs": [],
      "source": [
        "# Convert categorical columns to categorical\n",
        "categorical_cols = ['Year', 'Month', 'Day', 'Hour', 'Dayofweek', 'Dayofyear', 'Weekofyear']\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].astype('category')\n",
        "\n",
        "print(\"Forma de lo datos:\", df.shape)\n",
        "print(\"\\nTipo de datos por columnas:\\n\\n\", df.dtypes)\n",
        "print(\"\\nEstadisticas descriptivas:\\n\\n\", df.describe())\n",
        "print(\"\\nValores Faltantes:\\n\\n\", df.isnull().sum())\n",
        "print(\"\\nValores Faltantes(%):\\n\\n\", (df.isnull().sum() / len(df)) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjYYx9KjjuRY"
      },
      "source": [
        "se notan valores nulos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3l8_9rgre7O"
      },
      "source": [
        "## 3.2  Análisis de variables numericas y continuas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "yqqpWp6kF8_9",
        "outputId": "a928d5b8-8ee9-4309-b794-4079981b964c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Se define las columnas numericas a analizar\n",
        "numerical_columns = ['Consumption', 'Temperature', 'Humidity', 'Wind']\n",
        "\n",
        "# PRIMERO => Matriz de Correlacion\n",
        "# Se calcula la matriz de correlacion entre ellas\n",
        "correlation_matrix = df[numerical_columns].corr()\n",
        "\n",
        "# Configuracion del grafico para la matriz de correlación\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    correlation_matrix,\n",
        "    annot=True, #Muestra los valores de correlacion\n",
        "    cmap='RdYlGn', #Paleta de Colores(rojo-amarrillo-verde)\n",
        "    fmt='0.4f', #Formato de 4 decimales\n",
        "    vmin=-1, vmax=1, #Rango de valores para la escala de colores\n",
        ")\n",
        "plt.title('Correlation Matrix of Numerical Columns')\n",
        "plt.show()\n",
        "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "\n",
        "# SEGUNDO => Analisis de Distribucion y Relacion con Consumo\n",
        "# Se configura el diselo de subplots\n",
        "num_cols = 2 # Numero de columnas por fila\n",
        "num_rows = (len(numerical_columns)) # Una fila por variable\n",
        "\n",
        "# Crear la figura y los subplots\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows))\n",
        "axes = axes.flatten() # Convertir la matriz de subplots en una lista plana\n",
        "\n",
        "# Iteracion sobre cada columna numérica\n",
        "for i, col in enumerate(numerical_columns):\n",
        "    # Histograma: Distribucion de la variable\n",
        "    sns.histplot(df[col], bins=20, kde=True, ax=axes[i * 2], color='royalblue') # KDE para visualizar la distribución\n",
        "    axes[i * 2].set_title(f'Distribution of {col}', fontsize=14)\n",
        "    axes[i * 2].set_xlabel(col)\n",
        "    axes[i * 2].set_ylabel('Frequency')\n",
        "    \n",
        "    # Grafico de disperson: Relacion con CONSUMO\n",
        "    sns.regplot(\n",
        "        x=col,\n",
        "        y='Consumption',\n",
        "        data=df,\n",
        "        ax=axes[i * 2 + 1],\n",
        "        scatter_kws={'alpha': 0.5}, # Transparencia para mejorar la vista\n",
        "        line_kws={'color': 'red'}, # Color de la línea de tendencia\n",
        "    )\n",
        "    axes[i * 2 + 1].set_title(f'CONSUMO vs. {col}', fontsize=14)\n",
        "    axes[i * 2 + 1].set_xlabel(col) \n",
        "    axes[i * 2 + 1].set_ylabel('CONSUMO') \n",
        "    \n",
        "    # Calcular y mostrar la correlacion\n",
        "    correlation = df[col]. corr(df['Consumption'])\n",
        "    print(f\"Correlation between {col} and CONSUMO: {correlation:.4f}\")\n",
        "\n",
        "# Eliminar subplots no utilizados\n",
        "for j in range(len(numerical_columns) * 2, num_rows * num_cols):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5KkK9jpriLJ"
      },
      "source": [
        "## 3.3  Análisis de variables Categoricas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL_BSejjpJx-"
      },
      "source": [
        "Las columna fechas contiene información temporal (día, mes, año, hora, etc.), sin embargo, para que los diferentes modelos pueda aprovechar la información de la variable fecha, es necesario transformarla en características categóricas.\n",
        "A continuación, se trata la columna fecha:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7PMTtZprm2u"
      },
      "source": [
        "### Crear columna con **Dias festivos** en **Colombia**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "7EMij3Uqzb2D",
        "outputId": "f282ec01-dcc5-4fab-a753-84c882ed3342"
      },
      "outputs": [],
      "source": [
        "num_cols = 4\n",
        "num_rows = (len(categorical_cols) * 2 + num_cols - 1) // num_cols\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    sns.boxplot(x=col, y='Consumption', data=df, ax=axes[i])\n",
        "    axes[i].set_title(f'Consumption vs. {col}')\n",
        "\n",
        "    # Remove any unused subplots\n",
        "for j in range(len(categorical_cols), num_rows * num_cols):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY1UEVMcJ-02"
      },
      "source": [
        "- En la variable \"Year\" se puede evidenciar que los años 2020 y 2021 presentaron fenomenos sociales que impactaron directamente el consumo de eneriga en la ciudad de Cali. No existe en el dataset, una variable que permita modelar las restricciones a la movilidad en la pandemia y el paro nacional. *Se concidera prudente, sacar del estudio estos dos años.*\n",
        "- Se puede observar de \"Dayofweek\" que el consumo en los dias 1 a 4 tiene un comportamiento similar, en cambio los días 0, 5 y 6 se diferencian de los otros dias de la semana.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwMEHBMHrlkq"
      },
      "source": [
        "### Analisis de outliers y datos Atipicos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzEAlPE4IimZ"
      },
      "source": [
        "#### Eliminar datos atipicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBBfiVpAMjmo"
      },
      "source": [
        "Como se menciono antes, se procede a eliminar los datos de los años 2020 y 2021."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "collapsed": true,
        "id": "1Pr_KHoLLQ97",
        "outputId": "8a7c0c70-69aa-41f5-92e7-1e0ad89a9b18"
      },
      "outputs": [],
      "source": [
        "df = df.drop(df[(df['Year'] == 2020) | (df['Year'] == 2021)].index)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "num_cols = 4\n",
        "num_rows = (len(categorical_cols) * 2 + num_cols - 1) // num_cols\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    sns.countplot(x=col, data=df, ax=axes[i * 2])\n",
        "    axes[i * 2].set_title(f'Frequency Distribution of {col}')\n",
        "    sns.boxplot(x=col, y='Consumption', data=df, ax=axes[i * 2 + 1])\n",
        "    axes[i * 2 + 1].set_title(f'Consumption vs. {col}')\n",
        "\n",
        "for j in range(len(categorical_cols) * 2, num_rows * num_cols):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()  # Adjust subplot parameters to give specified padding.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q27YOue-MtOU"
      },
      "source": [
        "#### Analisis de Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-8-oDHbBOnXw",
        "outputId": "a8367ddb-7ea6-459e-ccfd-e6f7114114a6"
      },
      "outputs": [],
      "source": [
        "# Columnas numéricas\n",
        "numerical_cols = ['Consumption', 'Temperature', 'Humidity', 'Wind']\n",
        "# Box plots to visualize outliers\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    sns.boxplot(y=df[col])\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Descriptive statistics and IQR outlier detection\n",
        "for col in numerical_cols:\n",
        "    print(f\"\\nAnalysis for numerical variable: {col}\")\n",
        "    print(df[col].describe())\n",
        "\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    print(f\"Number of outliers: {len(outliers)}\")\n",
        "    print(f\"Lower bound: {lower_bound}, Upper bound: {upper_bound}\")\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=col, y='Consumption', data=df)\n",
        "    plt.title(f'Consumption vs. {col} with Outliers')\n",
        "\n",
        "    # Highlight outliers on the scatter plot\n",
        "    if not outliers.empty:\n",
        "        plt.scatter(outliers[col], outliers['Consumption'], color='red', label='Outliers')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1acDpstjuRa"
      },
      "source": [
        "**Conclusiones Outliers**\n",
        "- Se pudo observar un dato en la variable temperatura que se identificó como Outlier, sin embargo, este pudo obedecer a una temporada sin lluvia, en donde la temperatura se elevó a valores superiores.\n",
        "- Para la variable Consumo, de acuerdo a lo manifestado por los expertos del negocio, los valores bajos se puden considedar como outliers, causados por apagones o fallas en la red de distribucion de energia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "_wOlUDmWgOzL",
        "outputId": "954e1dc0-35c0-424a-af7d-7e18c81f11b4"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5CYtt8prsb-"
      },
      "source": [
        "# 4  Imputación de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duE7Lz3bP_TT",
        "outputId": "9cf1275f-a9a0-448c-c42d-ef703534d486"
      },
      "outputs": [],
      "source": [
        "# Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "df_cleaned = df.copy()\n",
        "\n",
        "# Crea series desplazadas para el valor anterior y siguiente\n",
        "temperatura_anterior = df_cleaned['Temperature'].shift(1)\n",
        "temperatura_siguiente = df_cleaned['Temperature'].shift(-1)\n",
        "\n",
        "# Calcula el promedio del valor anterior y siguiente\n",
        "promedio_anterior_siguiente = (temperatura_anterior + temperatura_siguiente) / 2\n",
        "\n",
        "# Imputa los valores faltantes con el promedio\n",
        "df_cleaned['Temperature'] = df_cleaned['Temperature'].fillna(promedio_anterior_siguiente)\n",
        "\n",
        "# Manejar los valores NaN restantes al inicio o al final del DataFrame\n",
        "df_cleaned['Temperature'] = df_cleaned['Temperature'].fillna(df_cleaned['Temperature'].median()) # Corregido\n",
        "\n",
        "# Reassign the cleaned DataFrame to df\n",
        "df = df_cleaned\n",
        "\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defino funcion enfocada para la ejecucion de los Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TemperatureImputer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        # No es necesario entrenar el modelo, así que simplemente retorna self\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        # Crear una copia del DataFrame para evitar la advertencia SettingWithCopyWarning\n",
        "        df_cleaned = X.copy()\n",
        "\n",
        "        # Crea series desplazadas para el valor anterior y siguiente\n",
        "        temperatura_anterior = df_cleaned['Temperature'].shift(1)\n",
        "        temperatura_siguiente = df_cleaned['Temperature'].shift(-1)\n",
        "\n",
        "        # Calcula el promedio del valor anterior y siguiente\n",
        "        promedio_anterior_siguiente = (temperatura_anterior + temperatura_siguiente) / 2\n",
        "\n",
        "        # Imputa los valores faltantes con el promedio\n",
        "        df_cleaned['Temperature'] = df_cleaned['Temperature'].fillna(promedio_anterior_siguiente)\n",
        "\n",
        "        # Manejar los valores NaN restantes al inicio o al final del DataFrame\n",
        "        df_cleaned['Temperature'] = df_cleaned['Temperature'].fillna(df_cleaned['Temperature'].median())\n",
        "\n",
        "        return df_cleaned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAbkELqxjuRb"
      },
      "source": [
        "- En el dataset estudiado no hay presencia de datos faltantes, se contempla el proceso de imputacion para la variable **Temperatura** debido a la posibilidad de la llegada de nuevos datos, a travez del cual se puede imputar los valores nulos.\n",
        "- Para imputar se tiene en cuenta la medida anterior y posterior con referencia al dato nulo, se realiza un promedio de estas dos medidas, teniendo en cuenta el vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkrf4VHjrvIS"
      },
      "source": [
        "## 4.1  Division de datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk0jd7Thf4S5"
      },
      "outputs": [],
      "source": [
        "y = df['Consumption']\n",
        "X = df.drop(['Consumption'], axis=1)\n",
        "\n",
        "print(\"primeras filas \\n\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"se describen las columnas \\n\")\n",
        "print(X.info())\n",
        "\n",
        "print(\"dimensiones \\n\")\n",
        "print(X.shape)\n",
        "print(\"dimensiones \\n\")\n",
        "print(y.shape)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKLnaAwerzG8"
      },
      "source": [
        "## 4.2  Estandarización y Escalamiento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBIA41ENg1lp"
      },
      "source": [
        "Teniendo en cuenta que solo la variable temperatura es numerica y continua no se requiere Escalar o normalizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKsTTX8UgeE1"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxvOOU-4r2BX"
      },
      "source": [
        "# 5  Feature Engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyx0MGtar3ou"
      },
      "source": [
        "## 5.1  Codificar Variables Categoricas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PXMkytXVpwB"
      },
      "source": [
        "Como las variables categoricas del dataset no tienen una cardinalidad, se usa la codificación One Hot Encoding, para su aplicación en el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4EUqn4Ohvfo",
        "outputId": "c3935e93-eb09-4946-c118-2c28ec2cb501"
      },
      "outputs": [],
      "source": [
        "# Use pd.get_dummies with the 'columns' parameter to specify which columns to dummify\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_cols)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_cols)\n",
        "#print(X_train.head())\n",
        "\n",
        "# Check if the number of columns in X_train and X_test are equal\n",
        "print(f\"Number of columns in X_train: {X_train.shape[1]} \\n\")\n",
        "print(f\"Number of columns in X_test: {X_test.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6 Regularización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Initialize the Lasso model with a regularization parameter alpha\n",
        "lasso_model = Lasso(alpha=0.1)\n",
        "\n",
        "# Train the model\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lasso = lasso_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
        "rmse_lasso = np.sqrt(mse_lasso)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Lasso Model - Mean Squared Error (MSE): {mse_lasso}\")\n",
        "print(f\"Lasso Model - R-squared (R2): {r2_lasso}\")\n",
        "print(f\"Lasso Model - Mean Absolute Error (MAE): {mae_lasso}\")\n",
        "print(f\"Lasso Model - Root Mean Squared Error (RMSE): {rmse_lasso}\")\n",
        "\n",
        "# Print the coefficients\n",
        "print('Lasso Coefficients:', lasso_model.coef_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar cuántos coeficientes no son cero\n",
        "num_nonzero_coefficients = np.sum(lasso_model.coef_ != 0)\n",
        "\n",
        "print(f\"Número de coeficientes no cero después de aplicar Lasso: {num_nonzero_coefficients}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Podemos observar que LASSO redujo notablemente los coeficientes de 252 a 78 Se puede observar que Lasso bajo la dimensionalidad de los datos, con 178 coeficientes iguales a cero.\n",
        "Lasso es útil para la selección de características.\n",
        "\n",
        "- Adicinalmente el $R^2$ de Lasso es menor que el de lo demás modelos, pero Lasso solo uso solo 78 coeficientes mientras que los otros dos usaron 252 coeficientes. El modelo Lasso es más factible porque las dimensiones son reducidas.\n",
        "\n",
        "-Por otro lado, se puede afirmar que el modelo de Ridge es mucho más costoso en términos operativos, y aunque los valores de los errores son muy similares , El método Lasso es mejor para la selección de características porque puede reducir los coeficientes sin importancia a cero, eliminando de manera efectiva esas características. Mientras que la regresión Ridge reduce todos los coeficientes pero no elimina ninguna característica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Initialize the Ridge model with a regularization parameter alpha\n",
        "ridge_model = Ridge(alpha=0.1)\n",
        "\n",
        "# Train the model\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
        "rmse_ridge = np.sqrt(mse_ridge)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Ridge Model - Mean Squared Error (MSE): {mse_ridge}\")\n",
        "print(f\"Ridge Model - R-squared (R2): {r2_ridge}\")\n",
        "print(f\"Ridge Model - Mean Absolute Error (MAE): {mae_ridge}\")\n",
        "print(f\"Ridge Model - Root Mean Squared Error (RMSE): {rmse_ridge}\")\n",
        "\n",
        "# Print the coefficients\n",
        "print('Ridge Coefficients:', ridge_model.coef_)\n",
        "# Count the number of coefficients that are not equal to 0\n",
        "num_nonzero_coefficients_ridge = np.sum(ridge_model.coef_ != 0)\n",
        "\n",
        "print(f\"Number of non-zero coefficients in Ridge model: {num_nonzero_coefficients_ridge}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic3-1aFzr-VE"
      },
      "source": [
        "# 7 Data Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline para probar el modelo de regresion random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#division del dataset para entrenamiento y prueba usando el pipeline\n",
        "X_train_to_pipe, X_test_to_pipe, y_train_to_pipe, y_test_to_pipe = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#codifico las variables categoricas\n",
        "X_train_to_pipe = pd.get_dummies(X_train_to_pipe, columns=categorical_cols)\n",
        "X_test_to_pipe = pd.get_dummies(X_test_to_pipe, columns=categorical_cols)\n",
        "\n",
        "\n",
        "# Pipeline para probar el modelo de regresion random forest\n",
        "pipe = Pipeline([\n",
        "  ('Imputacion temperatura', TemperatureImputer()),\n",
        "  ('Estandarizacion', StandardScaler()),\n",
        "  ('Model', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),)\n",
        "],verbose = True)\n",
        "\n",
        "\n",
        "# Entrenamiento del pipeline\n",
        "pipe.fit(X_train_to_pipe, y_train_to_pipe)\n",
        "\n",
        "\n",
        "# Evaluar el pipeline \n",
        "print(\"Accuracy:\",pipe.score(X_test_to_pipe, y_test_to_pipe))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline para la prueba de un modelo de regresión lineal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el pipeline con regresión lineal\n",
        "pipe = Pipeline([\n",
        "  ('Imputacion_temperatura', TemperatureImputer()),  \n",
        "  ('Estandarizacion', StandardScaler()), \n",
        "  ('model', LinearRegression())  \n",
        "], verbose=True)\n",
        "\n",
        "# Entrenamiento del pipeline\n",
        "pipe.fit(X_train_to_pipe, y_train_to_pipe)\n",
        "\n",
        "# Evaluar el pipeline\n",
        "print(\"Accuracy:\", pipe.score(X_test_to_pipe, y_test_to_pipe))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENLHlVusr-pH"
      },
      "source": [
        "# 7  Modelamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo hibrido XGBoost + DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. Define and train the DNN model\n",
        "dnn_model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "dnn_model.compile(optimizer='adam', loss='mse')\n",
        "dnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
        " \n",
        "# 2. Use the DNN model to generate predictions for the training and test sets\n",
        "dnn_train_predictions = dnn_model.predict(X_train)\n",
        "dnn_test_predictions = dnn_model.predict(X_test)\n",
        "\n",
        "# 3. Combine the original features with the DNN predictions\n",
        "X_train_hybrid = np.hstack((X_train, dnn_train_predictions))\n",
        "X_test_hybrid = np.hstack((X_test, dnn_test_predictions))\n",
        "\n",
        "# 4. Train the XGBoost model using the hybrid features\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "xgb_model.fit(X_train_hybrid, y_train)\n",
        "\n",
        "# 5. Make predictions with the XGBoost model\n",
        "y_pred_hybrid = xgb_model.predict(X_test_hybrid)\n",
        "\n",
        "# 6. Evaluate the hybrid model\n",
        "mse_hybrid = mean_squared_error(y_test, y_pred_hybrid)\n",
        "r2_hybrid = r2_score(y_test, y_pred_hybrid)\n",
        "mae_hybrid = mean_absolute_error(y_test, y_pred_hybrid)\n",
        "rmse_hybrid = np.sqrt(mse_hybrid)\n",
        "\n",
        "print(f\"Hybrid Model - Mean Squared Error (MSE): {mse_hybrid}\")\n",
        "print(f\"Hybrid Model - R-squared (R2): {r2_hybrid}\")\n",
        "print(f\"Hybrid Model - Mean Absolute Error (MAE): {mae_hybrid}\")\n",
        "print(f\"Hybrid Model - Root Mean Squared Error (RMSE): {rmse_hybrid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o5afeL2i3gB"
      },
      "source": [
        "### Modelo RegresionLineal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DWlyYg8imFG",
        "outputId": "a5a7b238-5771-4a56-c4f3-c7e5a6517f05"
      },
      "outputs": [],
      "source": [
        "# Initialize the Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"R-squared (R2): {r2}\")\n",
        "\n",
        "# Calculate MAE and MedAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "medae = median_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print('Intercepto: ', model.intercept_)\n",
        "print('Coeficientes: ', model.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the number of coefficients greater than 0\n",
        "num_positive_coefficients = np.sum(model.coef_ != 0)\n",
        "\n",
        "print(f\"Number of coefficients greater than 0: {num_positive_coefficients}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij1aJWMRi1Jf"
      },
      "source": [
        "### Modelo Randomforest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFWz_dDcinPa"
      },
      "outputs": [],
      "source": [
        "# Initialize the Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Random Forest - Mean Squared Error (MSE): {mse_rf}\")\n",
        "print(f\"Random Forest - R-squared (R2): {r2_rf}\")\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate the Median Absolute Error (MedAE)\n",
        "medae = median_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate the Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "\n",
        "# Print the calculated MAE, MedAE, and RMSE values.\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "527fPNfZM8uw"
      },
      "source": [
        "### Modelo de Redes Neuronales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T5A5jHvBM-6z",
        "outputId": "4bbb95e2-8f52-4f99-e2d2-c3effd8a7654"
      },
      "outputs": [],
      "source": [
        "# 1. Definir el modelo de la red neuronal\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(1)  # Capa de salida para regresión\n",
        "])\n",
        "\n",
        "# 2. Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse')  # Usamos 'mse' (error cuadrático medio) para regresión\n",
        "\n",
        "# 3. Entrenar el modelo\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# 4. Realizar predicciones\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Calcular métricas de evaluación\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'MSE: {mse}')\n",
        "print(f'MAE: {mae}')\n",
        "print(f'R2: {r2}')\n",
        "\n",
        "# Opcional: Graficar la pérdida durante el entrenamiento\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
