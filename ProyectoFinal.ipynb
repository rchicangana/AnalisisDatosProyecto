{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rchicangana/AnalisisDatosProyecto/blob/main/ProyectoFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVIuGSCNquud"
      },
      "source": [
        "# 1  Introduccion\n",
        "\n",
        "El análisis exploratorio de datos es una etapa fundamental en cualquier proyecto de Inteligencia Artificial y Ciencia de Datos, puesto que permite entender las características y patrones en los datos, así como identificar relaciones entre variables.\n",
        "\n",
        "A continuación se presenta un estudio basado en datos de consumo de energía eléctrica horaria registrados en Cali, Colombia, junto con las temperaturas correspondientes a cada hora. El objetivo principal es explorar la relación entre estas dos variables y, posteriormente, aplicar diferentes modelos para realizar el pronósticos de corto plazo del demanda de energía."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyGVSbOMq_0K"
      },
      "source": [
        "# 2  Contexto\n",
        "\n",
        "La demanda de energía eléctrica en una ciudad se ve inlfuenciada por muchos factores, como la temperatura, factores sociales, cronograma de produccion de grandes clientes, entre otros.\n",
        "\n",
        "En regiones con climas variables, como es el caso de muchas ciudades Colombianas, la demanda es afectada por el uso intensivo de sistemas de refrigeración en épocas calurosas.\n",
        "\n",
        "Los datos utilizados en este análisis incluyen mediciones horarias del consumo de energía y la temperatura registrada en la misma franja horaria, tipos de dia, mes y año. Lo que permite explorar la correlación entre estas variables.\n",
        "\n",
        "Adicionalmente, se busca evaluar el desempeño de diferentes modelos como regresiones lineales, redes neuronales y modelos basados en árboles de decisión, para predecir el consumo de energía en un horizonte de corto plazo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvcVK9n2rB4e"
      },
      "source": [
        "## 2.1  Descripción de los campos\n",
        "\n",
        "- FECHA: Fecha en formato ISO de la lectura del consumo\n",
        "- TEMPERATURA: Temperatura en grados Centigrados al momento de la toma del dato\n",
        "- CONSUMO: Variable objetvio medido en MVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2eSjd_Fz7WiZ",
        "outputId": "46271375-e6d0-45cd-80f7-ed2eccc38a00"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.inspection import permutation_importance\n",
        "import glob\n",
        "import holidays\n",
        "#importo el dataset Principal\n",
        "df = pd.read_csv('Data/datasetEmcali.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"primeras filas \\n\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "print(\"se describen las columnas \\n\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"dimensiones \\n\")\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUTgDLYErbbx"
      },
      "source": [
        "# 3  Análisis exploratorio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1t0tE05rdWs"
      },
      "source": [
        "## 3.1  Análisis de estructura\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSaKvv-pKkYL"
      },
      "source": [
        "Se realiza un analisis de la forma del dataset, se relacionan tipos de datos, estadisticas descriptivas y valores nulos.\n",
        "\n",
        "Se adicionan columnas, basadas en la fecha, con la hipotesis que esto permitira al modelo entender e interpretar mejor los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tratamiento de dataset Humedad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Defino el formato de la fecha\n",
        "df[\"FECHA\"] = pd.to_datetime(df[\"FECHA\"], format=\"mixed\", errors=\"coerce\")\n",
        "\n",
        "# Cargar datasets de humedad\n",
        "humedad_files = glob.glob(\"Data/reporte_sisaire-humedad-*.csv\")\n",
        "df_humedad_list = [pd.read_csv(file) for file in humedad_files]\n",
        "df_humedad = pd.concat(df_humedad_list)\n",
        "\n",
        "# Convertir fechas en el dataset de humedad\n",
        "df_humedad[\"Fecha inicial\"] = pd.to_datetime(df_humedad[\"Fecha inicial\"], format=\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "# Promediar la humedad por hora\n",
        "df_humedad_avg = df_humedad.groupby(\"Fecha inicial\")[\"HAire2\"].mean().reset_index()\n",
        "df_humedad_avg.rename(columns={\"Fecha inicial\": \"FECHA\", \"HAire2\": \"HUMIDITY\"}, inplace=True)\n",
        "\n",
        "df_humedad_avg.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tratamiento de dataset Temperatura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datasets de velocidad del viento\n",
        "viento_files = glob.glob(\"Data/reporte_sisaire-vViento-*.csv\")\n",
        "df_viento_list = [pd.read_csv(file) for file in viento_files]\n",
        "df_viento = pd.concat(df_viento_list)\n",
        "\n",
        "# Convertir fechas en el dataset de viento\n",
        "df_viento[\"Fecha inicial\"] = pd.to_datetime(df_viento[\"Fecha inicial\"], format=\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "# Promediar la velocidad del viento por hora\n",
        "df_viento_avg = df_viento.groupby(\"Fecha inicial\")[\"VViento\"].mean().reset_index()\n",
        "df_viento_avg.rename(columns={\"Fecha inicial\": \"FECHA\", \"VViento\": \"WIND\"}, inplace=True)\n",
        "\n",
        "df_viento_avg.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Union de los dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Unir los datos de humedad y viento con el dataset principal\n",
        "df_humedad_avg[\"FECHA\"] = pd.to_datetime(df_humedad_avg[\"FECHA\"])\n",
        "df_viento_avg[\"FECHA\"] = pd.to_datetime(df_viento_avg[\"FECHA\"])\n",
        "\n",
        "df = df.merge(df_humedad_avg, on=\"FECHA\", how=\"left\", suffixes=(\"\",\"_extra\"))\n",
        "df = df.merge(df_viento_avg, on=\"FECHA\", how=\"left\" , suffixes=(\"\",\"_extra\"))\n",
        "\n",
        "# Combinar las columnas duplicadas en una sola, tomando el promedio de los valores\n",
        "#df[\"HUMEDAD\"] = df[[\"HUMEDAD\", \"HUMEDAD_extra\"]].mean(axis=1)\n",
        "#df[\"VIENTO\"] = df[[\"VIENTO\", \"VIENTO_extra\"]].mean(axis=1)\n",
        "\n",
        "# Eliminar las columnas duplicadas\n",
        "#df = df.drop(columns=[col for col in df.columns if col.endswith(\"_extra\")])\n",
        "df = df.drop(columns=[col for col in df.columns if col.endswith(\"_x\")])\n",
        "df = df.drop(columns=[col for col in df.columns if col.endswith(\"_y\")])\n",
        "\n",
        "print(\"primeras filas \\n\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "print(\"se describen las columnas \\n\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"dimensiones \\n\")\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HXUvfE-D9Ngq",
        "outputId": "37761333-4627-4ab1-fb14-69c3439fb775"
      },
      "outputs": [],
      "source": [
        "\n",
        "# le doy formato a las columnas que va a ser numericas\n",
        "df['CONSUMO'] = pd.to_numeric(df['CONSUMO'], errors='coerce')\n",
        "\n",
        "df['TEMPERATURA'] = pd.to_numeric(df['TEMPERATURA'], errors='coerce')\n",
        "\n",
        "df['HUMIDITY'] = pd.to_numeric(df['HUMIDITY'], errors='coerce')\n",
        "\n",
        "df['WIND'] = pd.to_numeric(df['WIND'], errors='coerce')\n",
        "\n",
        "# Elimino las filas que tienen valores nulos en 'FECHA' y 'CONSUMO', 'TEMPERATURA', 'HUMEDAD', 'VIENTO'\n",
        "\n",
        "df = df.dropna(subset=['FECHA', 'CONSUMO', 'TEMPERATURA', 'HUMIDITY', 'WIND'])\n",
        "    \n",
        "\n",
        "\n",
        "def es_festivo_colombia(fecha):\n",
        "    \"\"\"Verifica si una fecha es festivo en Colombia.\"\"\"\n",
        "    co_holidays = holidays.CO(years=fecha.year)\n",
        "    return fecha in co_holidays\n",
        "df['Is_holiday'] = df['FECHA'].apply(es_festivo_colombia)\n",
        "\n",
        "# Extract numerical features from 'FECHA'\n",
        "df['Year'] = df['FECHA'].dt.year\n",
        "df['Month'] = df['FECHA'].dt.month\n",
        "df['Day'] = df['FECHA'].dt.day\n",
        "df['Hour'] = df['FECHA'].dt.hour\n",
        "df['Dayofweek'] = df['FECHA'].dt.dayofweek\n",
        "df['Dayofyear'] = df['FECHA'].dt.dayofyear\n",
        "df['Weekofyear'] = df['FECHA'].dt.isocalendar().week\n",
        "df['Is_month_end'] = df['FECHA'].dt.is_month_end\n",
        "df['Is_month_start'] = df['FECHA'].dt.is_month_start\n",
        "df['Is_quarter_end'] = df['FECHA'].dt.is_quarter_end\n",
        "df['Is_quarter_start'] = df['FECHA'].dt.is_quarter_start\n",
        "df['Is_year_end'] = df['FECHA'].dt.is_year_end\n",
        "df['Is_year_start'] = df['FECHA'].dt.is_year_start\n",
        "\n",
        "# Eliminar la columna 'FECHA'\n",
        "df = df.drop('FECHA', axis=1)\n",
        "\n",
        "# Convert categorical columns to categorical\n",
        "categorical_cols = ['Year', 'Month', 'Day', 'Hour', 'Dayofweek', 'Dayofyear', 'Weekofyear']\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].astype('category')\n",
        "\n",
        "print(\"Forma de lo datos:\", df.shape)\n",
        "print(\"\\nTipo de datos por columnas:\\n\\n\", df.dtypes)\n",
        "print(\"\\nEstadisticas descriptivas:\\n\\n\", df.describe())\n",
        "print(\"\\nValores Faltantes:\\n\\n\", df.isnull().sum())\n",
        "print(\"\\nValores Faltantes(%):\\n\\n\", (df.isnull().sum() / len(df)) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "se notan valores nulos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3l8_9rgre7O"
      },
      "source": [
        "## 3.2  Análisis de variables numericas y continuas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "yqqpWp6kF8_9",
        "outputId": "18206bc0-7226-4f20-a993-4557f61d1212"
      },
      "outputs": [],
      "source": [
        "\n",
        "numerical_columns = ['CONSUMO', 'TEMPERATURA', 'HUMIDITY', 'WIND']\n",
        "\n",
        "correlation_matrix = df[numerical_columns].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "#Matriz de correlación del DataFrame con valores de correlación entre -1 y 1, utilizando una paleta de colores verde-amarillo-rojo para resaltar las relaciones positivas y negativas, respectivamente.\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='RdYlGn', fmt=\"0.4f\")\n",
        "\n",
        "plt.title('Correlation Matrix of Numerical Columns')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "continuous_cols = ['TEMPERATURA', 'CONSUMO', 'HUMIDITY', 'WIND']\n",
        "\n",
        "num_cols = 2  # columns per row\n",
        "num_rows = (len(continuous_cols) * 2 + num_cols - 1) // num_cols # Calculate the rows needed.\n",
        "\n",
        "# Create subplots in a grid\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows))  # Adjust figsize as needed\n",
        "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "for i, col in enumerate(continuous_cols):\n",
        "    # Histogram\n",
        "    axes[i * 2].hist(df[col], bins=20)\n",
        "    axes[i * 2].set_title(f'Distribution of {col}')\n",
        "\n",
        "    # Scatterplot\n",
        "    sns.scatterplot(x=col, y='CONSUMO', data=df, ax=axes[i * 2 + 1])\n",
        "    axes[i * 2 + 1].set_title(f'CONSUMO vs. {col}')\n",
        "\n",
        "    correlation = df[col].corr(df['CONSUMO'])\n",
        "    print(f\"Correlation between {col} and CONSUMO: {correlation}\")\n",
        "\n",
        "# Remove any unused subplots\n",
        "for j in range(len(continuous_cols) * 2, num_rows * num_cols):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5KkK9jpriLJ"
      },
      "source": [
        "## 3.3  Análisis de variables Categoricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ukTYLniNI4_w",
        "outputId": "d7f34f83-a24a-4a60-a8e4-f2e1d1f457bd"
      },
      "outputs": [],
      "source": [
        "# Assuming 'df' is your DataFrame and 'categorical_cols' is defined\n",
        "#categorical_cols = ['Year', 'Month', 'Day', 'Hour', 'Dayofweek', 'Dayofyear', 'Weekofyear']\n",
        "\n",
        "# Determine the number of rows needed for the subplots\n",
        "num_cols = 4  # 4 columns per row\n",
        "num_rows = (len(categorical_cols) * 2 + num_cols - 1) // num_cols # Calculate the rows needed.\n",
        "\n",
        "# Create subplots in a grid\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows))  # Adjust figsize as needed\n",
        "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    # Countplot\n",
        "    sns.countplot(x=col, data=df, ax=axes[i * 2])\n",
        "    axes[i * 2].set_title(f'Frequency Distribution of {col}')\n",
        "    axes[i * 2].tick_params(axis='x', rotation=45)  # Rotate x-axis labels if needed\n",
        "\n",
        "    # Boxplot\n",
        "    sns.boxplot(x=col, y='CONSUMO', data=df, ax=axes[i * 2 + 1])\n",
        "    axes[i * 2 + 1].set_title(f'CONSUMO vs. {col}')\n",
        "    axes[i * 2 + 1].tick_params(axis='x', rotation=45)  # Rotate x-axis labels if needed\n",
        "\n",
        "# Remove any unused subplots\n",
        "for j in range(len(categorical_cols) * 2, num_rows * num_cols):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()  # Adjust subplot parameters to give specified padding.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY1UEVMcJ-02"
      },
      "source": [
        "Los años 2020 y 2021 presentaron un fenomeno social el cual no es posible modelar en este espacio. Las restricciones a la movilidad en la pandemia y el paro nacional, impactaron directamente el consumo de eneriga en la ciudad de Cali.\n",
        "\n",
        "Se concidera prudente, sacar del estudio estos dos años."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "1Pr_KHoLLQ97",
        "outputId": "ac38f42f-333c-4054-8d76-4ad3b254d333"
      },
      "outputs": [],
      "source": [
        "df = df.drop(df[(df['Year'] == 2020) | (df['Year'] == 2021)].index)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "num_cols = 4  # 4 columns per row\n",
        "num_rows = (len(categorical_cols) * 2 + num_cols - 1) // num_cols # Calculate the rows needed.\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows))  # Adjust figsize as needed\n",
        "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
        "\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    sns.countplot(x=col, data=df, ax=axes[i * 2])\n",
        "    axes[i * 2].set_title(f'Frequency Distribution of {col}')\n",
        "    axes[i * 2].tick_params(axis='x', rotation=45)  # Rotate x-axis labels if needed\n",
        "\n",
        "    sns.boxplot(x=col, y='CONSUMO', data=df, ax=axes[i * 2 + 1])\n",
        "    axes[i * 2 + 1].set_title(f'CONSUMO vs. {col}')\n",
        "    axes[i * 2 + 1].tick_params(axis='x', rotation=45)  # Rotate x-axis labels if needed\n",
        "for j in range(len(categorical_cols) * 2, num_rows * num_cols):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()  # Adjust subplot parameters to give specified padding.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwMEHBMHrlkq"
      },
      "source": [
        "### 3.4.1  Analisis de outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "-8-oDHbBOnXw",
        "outputId": "086a808c-c413-4896-d0bb-33d2d2783702"
      },
      "outputs": [],
      "source": [
        "# Columnas numéricas\n",
        "numerical_cols = ['TEMPERATURA', 'CONSUMO', 'HUMIDITY', 'WIND']\n",
        "# Box plots to visualize outliers\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    sns.boxplot(y=df[col])\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Descriptive statistics and IQR outlier detection\n",
        "for col in numerical_cols:\n",
        "    print(f\"\\nAnalysis for numerical variable: {col}\")\n",
        "    print(df[col].describe())\n",
        "\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    print(f\"Number of outliers: {len(outliers)}\")\n",
        "    print(f\"Lower bound: {lower_bound}, Upper bound: {upper_bound}\")\n",
        "    # Document findings on potential causes and consequences of outliers (example)\n",
        "    if col == 'TEMPERATURA':\n",
        "        print(\"Potential causes for outliers in 'TEMPERATURA': Sensor malfunction, extreme weather events\")\n",
        "        print(\"Consequences: Skewed temperature distributions, inaccurate consumption predictions\")\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=col, y='CONSUMO', data=df)\n",
        "    plt.title(f'CONSUMO vs. {col} with Outliers')\n",
        "\n",
        "    # Highlight outliers on the scatter plot\n",
        "    if not outliers.empty:\n",
        "        plt.scatter(outliers[col], outliers['CONSUMO'], color='red', label='Outliers')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnTW3muRrq3c"
      },
      "source": [
        "### 3.4.2  Conclusiones Outliers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- En el contexto energetico y de negocion, los bajos consumos se puden considedar como outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "_wOlUDmWgOzL",
        "outputId": "44400bf3-0855-45db-a0d5-4984ec56be35"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5CYtt8prsb-"
      },
      "source": [
        "# 4  Limpieza de datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGpQCMjnr0lo"
      },
      "source": [
        "## 4.2  Imputación\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duE7Lz3bP_TT",
        "outputId": "9cf1275f-a9a0-448c-c42d-ef703534d486"
      },
      "outputs": [],
      "source": [
        "# Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "df_cleaned = df.copy()\n",
        "\n",
        "# Crea series desplazadas para el valor anterior y siguiente\n",
        "temperatura_anterior = df_cleaned['TEMPERATURA'].shift(1)\n",
        "temperatura_siguiente = df_cleaned['TEMPERATURA'].shift(-1)\n",
        "\n",
        "# Calcula el promedio del valor anterior y siguiente\n",
        "promedio_anterior_siguiente = (temperatura_anterior + temperatura_siguiente) / 2\n",
        "\n",
        "# Imputa los valores faltantes con el promedio\n",
        "df_cleaned['TEMPERATURA'] = df_cleaned['TEMPERATURA'].fillna(promedio_anterior_siguiente)\n",
        "\n",
        "# Manejar los valores NaN restantes al inicio o al final del DataFrame\n",
        "df_cleaned['TEMPERATURA'] = df_cleaned['TEMPERATURA'].fillna(df_cleaned['TEMPERATURA'].median()) # Corregido\n",
        "\n",
        "# Reassign the cleaned DataFrame to df\n",
        "df = df_cleaned\n",
        "\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- se contempla el proceso debido a la posibilidad de la llegada de nuevos datos, a travez del cual se pueden imputan lo nuevos datos nulos. el proceso de imputacion se esat llevando a cabo teniendo en cuenta la medida anterior y posterior con referencia al dato nulo, se realiza un promedio de estas dos medidas debido a que las variaciones de la temperatura en el rango de 1 hora es minimo.\n",
        "- ademas se elimina la fecha porque ya no es relevante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkrf4VHjrvIS"
      },
      "source": [
        "## 4.1  Division de datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk0jd7Thf4S5"
      },
      "outputs": [],
      "source": [
        "y = df['CONSUMO']\n",
        "X = df.drop(['CONSUMO'], axis=1)\n",
        "\n",
        "print(\"primeras filas \\n\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"se describen las columnas \\n\")\n",
        "print(X.info())\n",
        "\n",
        "print(\"dimensiones \\n\")\n",
        "print(X.shape)\n",
        "print(\"dimensiones \\n\")\n",
        "print(y.shape)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKLnaAwerzG8"
      },
      "source": [
        "## 4.2  Estandarización y Escalamiento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBIA41ENg1lp"
      },
      "source": [
        "Teniendo en cuenta que solo la variable temperatura es numerica y continua no se requiere Escalar o normalizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKsTTX8UgeE1"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxvOOU-4r2BX"
      },
      "source": [
        "# 5  Feature Engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyx0MGtar3ou"
      },
      "source": [
        "## 5.1  Dummificar variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4EUqn4Ohvfo",
        "outputId": "0ad8ade0-9604-44bd-a788-39fe34f2908a"
      },
      "outputs": [],
      "source": [
        "# Use pd.get_dummies with the 'columns' parameter to specify which columns to dummify\n",
        "\n",
        "print(X_train.head())\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_cols)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_cols)\n",
        "\n",
        "# Check if the number of columns in X_train and X_test are equal\n",
        "print(f\"Number of columns in X_train: {X_train.shape[1]} \\n\")\n",
        "print(f\"Number of columns in X_test: {X_test.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkmP9Z0Qr5Z3"
      },
      "source": [
        "## 5.2  PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1BcC8dhj6zs",
        "outputId": "c3596bc7-6597-41ae-a356-af2895549b2e"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing/validation sets\n",
        "X_train_final, X_test_val, y_train_final, y_test_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# # Split testing/validation set into testing and validation sets\n",
        "X_test, X_val, y_test, y_val = train_test_split(\n",
        "    X_test_val, y_test_val, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"X_train_final shape: {X_train_final.shape}, y_train_final shape: {y_train_final.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "1jejxbdUjVkD",
        "outputId": "d2697d55-2a0a-42a0-9a27-3e33575b0086"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize PCA with 95% variance retained\n",
        "pca = PCA(n_components=0.95)\n",
        "\n",
        "# Fit PCA on the training data\n",
        "pca.fit(X_train_final)\n",
        "\n",
        "# Transform the training, testing, and validation data\n",
        "X_train_pca = pca.transform(X_train_final)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "X_val_pca = pca.transform(X_val)\n",
        "\n",
        "# Print the explained variance ratio\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "# Replace original data with PCA-transformed data\n",
        "X_train_final = X_train_pca\n",
        "X_test = X_test_pca\n",
        "X_val = X_val_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCMWA4spr63k"
      },
      "source": [
        "## 5.3  LDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYEIsYPor8Y1"
      },
      "source": [
        "## 5.4  Clustering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No aplica, Colocar Explicacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic3-1aFzr-VE"
      },
      "source": [
        "# 6 Data Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scoring data\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# importing pipes for making the Pipe flow\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "pipe = Pipeline([\n",
        "  ('Estandarizacion', StandardScaler()),\n",
        "  ('model', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),)\n",
        "],verbose = True)\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# y_pred = pipe.predict(X_test)\n",
        "\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "#print(\"Accuracy:\", accuracy_score(y_test, pipe.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENLHlVusr-pH"
      },
      "source": [
        "# 7  Modelamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o5afeL2i3gB"
      },
      "source": [
        "RegresionLineal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DWlyYg8imFG",
        "outputId": "7418c5e8-c207-4956-9ec1-f1ce6ef14a80"
      },
      "outputs": [],
      "source": [
        "# Initialize the Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"R-squared (R2): {r2}\")\n",
        "\n",
        "# Calculate MAE and MedAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "medae = median_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij1aJWMRi1Jf"
      },
      "source": [
        "Randomforest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFWz_dDcinPa",
        "outputId": "a22c8e57-35b2-4a58-92a4-7419e92600a3"
      },
      "outputs": [],
      "source": [
        "# Initialize the Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_final, y_train_final)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Random Forest - Mean Squared Error (MSE): {mse_rf}\")\n",
        "print(f\"Random Forest - R-squared (R2): {r2_rf}\")\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate the Median Absolute Error (MedAE)\n",
        "medae = median_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate the Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "\n",
        "# Print the calculated MAE, MedAE, and RMSE values.\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neMsSylpjGBX"
      },
      "source": [
        "Hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1P61HkTjG8y"
      },
      "outputs": [],
      "source": [
        "# Define la cuadrícula de hiperparámetros para buscar\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15],\n",
        "}\n",
        "\n",
        "# Crea un objeto GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Ajusta el GridSearchCV a los datos de entrenamiento\n",
        "grid_search.fit(X_train_final, y_train_final)\n",
        "\n",
        "# Obtén el mejor modelo\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evalúa el mejor modelo en los datos de prueba\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "mse_best = mean_squared_error(y_test, y_pred_best)\n",
        "r2_best = r2_score(y_test, y_pred_best)\n",
        "\n",
        "# Imprime las métricas de evaluación\n",
        "print(f\"Mejor modelo - Error cuadrático medio (MSE): {mse_best}\")\n",
        "print(f\"Mejor modelo - R-cuadrado (R2): {r2_best}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "527fPNfZM8uw"
      },
      "source": [
        "Red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T5A5jHvBM-6z",
        "outputId": "3128e6e4-8236-4683-c1fe-4fc697281e61"
      },
      "outputs": [],
      "source": [
        "# 1. Definir el modelo de la red neuronal\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(1)  # Capa de salida para regresión\n",
        "])\n",
        "\n",
        "# 2. Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse')  # Usamos 'mse' (error cuadrático medio) para regresión\n",
        "\n",
        "# 3. Entrenar el modelo\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# 4. Realizar predicciones\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Calcular métricas de evaluación\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'MSE: {mse}')\n",
        "print(f'MAE: {mae}')\n",
        "print(f'R2: {r2}')\n",
        "\n",
        "# Opcional: Graficar la pérdida durante el entrenamiento\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZEFmzRHNALy"
      },
      "source": [
        "Red neuronal con LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "RT6_1ENcT-Mn",
        "outputId": "684a847e-5b25-4f57-d045-1c492053e048"
      },
      "outputs": [],
      "source": [
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LiFWAHohNDJo",
        "outputId": "c8d3dea5-aa6b-4a28-dd60-b2f4bf3e5e5b"
      },
      "outputs": [],
      "source": [
        "# 1. Preparar datos para LSTM (reshape a [muestras, pasos de tiempo, características])\n",
        "X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# 2. Definir el modelo LSTM\n",
        "model = keras.Sequential([\n",
        "    keras.layers.LSTM(50, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 3. Compilar y entrenar el modelo\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, verbose=1)\n",
        "\n",
        "# 4. Realizar predicciones\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "\n",
        "# 5. Evaluar el modelo\n",
        "# (calcula métricas como MSE, MAE, etc.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMcxLzKZwsZgGNGE8K+ljoJ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_AD",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
